---
title: "Tutorial Web Scrapping dan Teks Mining " 
author: "Muhammad I. Lubis (ilubis85@gmail.com)"
output: github_document
link-citations: yes
always_allow_html: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

<!-- badges: start -->

<!-- badges: end -->

## I. Apa itu web scrapping
Berdasarkan halaman [Wikipedia](https://en.wikipedia.org/wiki/Web_scraping), web scraping merujuk pada proses ekstraksi data secara otomatis dari situs web. Dengan menggunakan beberapa baris *scripts*  web scraping memungkinkan pengguna untuk mengambil dan menguraikan halaman HTML atau data terstruktur lainnya dari situs web, serta mengekstrak informasi yang diinginkan. Dengan web scraping, pengguna dapat mengumpulkan data dari berbagai halaman web, mengubahnya ke dalam format yang lebih terstruktur, dan kemudian melakukan analisis atau menggunakan data tersebut untuk berbagai tujuan.

## II. Apa tujuan atau kegunaan dari web scraping?
Web scraping adalah proses ekstraksi data dari situs web. Setelah data dikumpulkan melalui web scraping, data tersebut dapat dianalisis dalam berbagai cara tergantung pada jenis data dan tujuan analisis. Berikut adalah beberapa jenis analisis umum yang dapat dilakukan menggunakan data web scraping:

1. Analisis Sentimen: Web scraping dapat digunakan untuk mengumpulkan data dari platform media sosial, situs ulasan, atau artikel berita, yang kemudian dapat dianalisis untuk menentukan sentimen terhadap produk, merek, atau topik tertentu. Teknik analisis sentimen dapat membantu mengidentifikasi sentimen positif, negatif, atau netral yang terungkap dalam data yang di-scraper.

2. Riset Pasar: Web scraping dapat digunakan untuk mengumpulkan data tentang harga, fitur produk, ulasan pelanggan, atau informasi pesaing dari situs e-commerce. Data ini dapat digunakan untuk tujuan riset pasar seperti pemantauan harga, analisis tren, atau mengidentifikasi kesenjangan di pasar.

3. Analisis Konten: Web scraping dapat digunakan untuk mengekstraksi data teks dari situs web, seperti artikel berita, pos blog, atau diskusi forum. Data ini dapat dianalisis menggunakan teknik seperti pemrosesan bahasa alami (natural language processing/NLP) untuk mengidentifikasi pola, topik, atau kata kunci yang menarik. Analisis konten dapat digunakan untuk tujuan riset, pemantauan tren, atau memahami pendapat pelanggan.

4. Analisis Jaringan Sosial: Web scraping dapat digunakan untuk mengumpulkan data dari platform media sosial atau komunitas online. Data ini dapat digunakan untuk analisis jaringan sosial, yang melibatkan pemeriksaan hubungan antara individu, kelompok, atau organisasi. Analisis jaringan sosial dapat memberikan wawasan tentang aliran informasi, pengaruh, atau dinamika komunitas.

5. Visualisasi Data: Data hasil web scraping dapat divisualisasikan menggunakan berbagai alat dan teknik. Visualisasi dapat membantu mengidentifikasi pola, tren, atau anomali dalam data. Representasi visual seperti grafik, diagram, atau peta dapat memudahkan interpretasi dan komunikasi wawasan yang diperoleh dari data yang di-scraper.

6. Pembelajaran Mesin dan Pemodelan Prediktif: Data hasil web scraping dapat menjadi sumber daya berharga untuk melatih model pembelajaran mesin atau membangun model prediktif. Dengan mengumpulkan jumlah data yang besar, termasuk fitur-fitur yang menarik, web scraping memungkinkan pengembangan model yang dapat memprediksi hasil, mengklasifikasikan data, atau memberikan rekomendasi.

7. Analisis Kompetitif: Web scraping dapat digunakan untuk mengumpulkan data tentang situs web pesaing, daftar produk, informasi harga, atau strategi pemasaran. Data ini dapat memberikan wawasan tentang posisi pasar, penawaran pesaing, atau strategi penetapan harga, membantu bisnis membuat keputusan yang terinformasi dan mendapatkan keunggulan kompetitif.

Itulah beberapa contoh jenis analisis yang dapat dilakukan menggunakan data web scraping. Kemungkinannya sangat luas, dan teknik analisis yang digunakan secara spesifik

## III. Apakah web scraping legal?
Pertanyaan yang sering muncul adalah apakah kegiatan web scraping legal atau tidak. Berdasarkan informasi terbaru dari [APIFYBlog](https://blog.apify.com/is-web-scraping-legal/#:~:text=can%20scrape%20everything.-,Myth%202%3A%20Web%20scrapers%20operate%20in%20a%20grey%20area%20of,not%20heavily%20regulated%2C%20that's%20true.), web scraping legal selama informasi yang diakses adalah informasi publik dan bukan informasi yang bersifat pribadi atau merupakan kekayaan intelektual. Secara analogi, web scraping bisa diibaratkan seperti mengambil foto, di mana semua boleh difoto kecuali dokumen rahasia atau lokasi-lokasi yang dilarang untuk difoto..

```{r, out.width='70%', fig.align='center', fig.cap='Mitos terkait web scraping', echo=FALSE, warning=TRUE}
knitr::include_graphics('E:/myRpackages/Webscraping_tutorial/docs/Webscraping_legal.jpg')
```

## IV. Bagaimana melakukan web scrapping
Web scraping dapat dilakukan melalui dua tahap: pertama, mencari dan mengumpulkan alamat website, dan kedua, melakukan ekstraksi informasi yang dibutuhkan dengan membangun kode atau skrip. Jika Anda menggunakan R, Anda dapat menggunakan library **rvest** untuk melakukan ekstraksi data. Sebagai contoh,  kita akan melakukan scraping informasi dari artikel online terkait konflik antara manusia dan harimau (KMH).

#### a. Kumpulkan beberapa artikel online untuk di-scraping
Konflik manusia dan harimau (KMH) menunjukkan tren peningkatan dalam beberapa tahun terakhir, terutama di daerah dengan tingkat tutupan hutan yang tinggi, seperti di Provinsi Aceh. Saat ini, data terkait sebaran lokasi konflik antara manusia dan harimau tidak tersedia secara umum, padahal informasi tersebut sangat penting untuk mitigasi dan pengendalian konflik guna meminimalkan dampak negatif terhadap manusia dan satwa liar. 

Salah satu sumber informasi publik terkait KMH dapat diakses melalui berita atau artikel online. Tutorial web scraping ini akan mencoba mengekstrak informasi konflik melalui media online. Kita akan menggunakan link berikut ini sebagai contoh

"https://aceh.tribunnews.com/2020/03/11/teror-harimau-sumatera-belum-mereda-di-subulussalam-giliran-desa-bawan-jadi-sasaran?page=all"

Untuk melakukan web scraping pada halaman tersebut, kita akan mengekstraksi informasi yang dibutuhkan, seperti judul artikel, tanggal publikasi, dan isi artikel.

```{r echo=FALSE}
Website <- c("TribunNews", "TribunNews", "TribunNews", "KompasNews", "KompasNews")

alamat_website <- c("https://aceh.tribunnews.com/2020/03/11/teror-harimau-sumatera-belum-mereda-di-subulussalam-giliran-desa-bawan-jadi-sasaran?page=all",
         "https://aceh.tribunnews.com/2020/10/05/harimau-berkeliaran-di-objek-wisata-kempra-dampak-tidak-adanya-kajian-lingkungan-spot-baru",
         "https://aceh.tribunnews.com/2020/03/11/warga-pastikan-harimau-sumatera-yang-kembali-meneror-mangsa-ternak?page=all",
         "https://regional.kompas.com/read/2018/11/15/16461841/masuk-ke-kampung-kawanan-harimau-sumatera-terkam-3-ekor-kerbau-warga?page=all",
         "https://regional.kompas.com/read/2019/12/07/14385741/ternyata-ini-penyebab-harimau-masuk-ke-permukiman-dan-mangsa-ternak-warga?page=all")

berita_htc <- data.frame(Website, alamat_website)
colnames(berita_htc) <- c("Website", "alamat_website")
```

#### b. Gunakan SelectorGadget untuk mengetahui kode lokasi beberapa informasi
SelectorGadget adalah salah satu ekstensi untuk web browser Chrome yang sangat berguna dalam menentukan kode atau lokasi dari informasi yang ingin diekstrak. Ini memungkinkan Anda dengan mudah memilih elemen HTML yang ingin Anda tuju untuk scraping, seperti tanggal publikasi, penulis artikel, atau isi tulisan. Penting untuk diingat bahwa setiap website, bahkan setiap halaman dalam satu website, mungkin memiliki kode dan lokasi yang berbeda-beda.

**Berikut adalah langkah-langkah penggunaan SelectorGadget:**

1. Buka halaman web yang ingin Anda scrape di browser Chrome.
2. Install ekstensi SelectorGadget dari Chrome Web Store.
3. Setelah ekstensi terpasang, klik ikon SelectorGadget di toolbar Chrome (ikon berbentuk cangkul).
4. Saat mode SelectorGadget aktif, elemen yang Anda arahkan dengan kursor akan diberi penyorotan.
5. Klik pada elemen yang ingin Anda pilih. SelectorGadget akan mencoba memilih elemen tersebut bersama dengan kode selektornya.
6. Jika elemen yang dipilih tidak sesuai atau tidak akurat, klik lagi pada elemen lain untuk memperbaikinya. Anda dapat mengklik beberapa elemen untuk memilih area yang tepat.
7. Setelah Anda puas dengan pemilihan elemen, Anda dapat melihat kode selektor di bagian atas jendela SelectorGadget. Anda dapat menyalin dan menggunakan kode tersebut dalam skrip web scraping Anda.
8. Ulangi langkah-langkah di atas untuk setiap informasi yang ingin Anda ekstrak, seperti tanggal, penulis, atau isi tulisan.

Dengan menggunakan SelectorGadget, Anda dapat dengan mudah mengetahui kode atau lokasi elemen-elemen yang ingin Anda scrape pada halaman web yang dituju, seperti contoh pada gambar berikut. 

```{r, out.width='100%', fig.align='center', fig.cap='Membedah website dengan SelectorGadget', echo=FALSE, warning=TRUE}
knitr::include_graphics('E:/myRpackages/Webscraping_tutorial/docs/SelectorGadget_ss.png')
```

#### c. Instal program R dan R studio serta library yang diperlukan untuk web craping
1. Instal R:
Unduh dan instal R dari situs [resmi](https://www.r-project.org/).
Pilih versi yang sesuai untuk sistem operasi Anda (Windows, macOS, Linux) dan ikuti petunjuk instalasi.

2. Instal RStudio (Opsional, tetapi Direkomendasikan):
RStudio adalah lingkungan pengembangan terpadu (IDE) untuk R yang menyediakan *interface* yang lebih ramah pengguna dan fitur tambahan. Sangat disarankan untuk bekerja dengan R.
Unduh dan instal RStudio dari situs [resmi](https://www.rstudio.com/products/rstudio/download/).
Pilih versi gratis (RStudio Desktop) yang sesuai untuk sistem operasi Anda dan ikuti petunjuk instalasi.

3. Instal *library* atau *package* yang diperlukan:
Web scraping dalam R umumnya membutuhkan beberapa paket. Buka R atau RStudio dan instal paket yang diperlukan dengan menjalankan perintah berikut:
```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(rvest) # Untuk web scraping
library(tidyverse) # Untuk data cleaning
```
Dengan demikian, Anda dapat mulai menulis dan menjalankan kode web scraping menggunakan paket yang telah diinstal

##### i. Menulis kode sederhana untuk melakukan web scraping dari satu halaman situs
Dengan menggunakan *selectorGadget*, kita dapat mengetahui kode website yang dituju terkait lokasi **tanggal artikel terbit**, **judul artikel** dan **isi artikel** :
```{r, echo=TRUE, warning=FALSE}
# Membaca alamat website
url <- rvest::read_html("https://aceh.tribunnews.com/2020/03/11/teror-harimau-sumatera-belum-mereda-di-subulussalam-giliran-desa-bawan-jadi-sasaran?page=all")

# Membaca tanggal terbit artikel
tanggal <- url %>% # Membaca alamat website
  rvest::html_node("time") %>% # Membaca kode tanggal
  rvest::html_text() # Mengubah ke dalam bentuk text
tanggal

# Membaca judul artikel
judul <- url %>% # Membaca alamat website
  rvest::html_node("#arttitle") %>% # Membaca kode judul artikel
  rvest::html_text() # Mengubah ke dalam format text
judul

# Membaca isi artikel
berita <- url %>% # Membaca alamat website
  rvest::html_nodes("p") %>% # Membaca kode isi berita artikel
  rvest::html_text() %>% # Mengubah ke dalam format text
  {gsub('[\r\n\t\"]', '',.)} # Menghilangkan karakter yang tidak perlu
head(berita, 5) # Hanya menampilkan 5 kalimat pertama

```
Saat melakukan web scraping pada isi berita, seringkali kita menemui tautan-tautan yang tidak terkait dengan konten berita tersebut. Menghapus tautan-tautan semacam itu bisa menjadi tantangan dalam web scraping. Sebab, tautan-tautan tersebut dapat memiliki pola atau karakteristik yang berbeda-beda, sehingga menghapusnya secara otomatis menjadi sulit.

##### ii. Menulis kode untuk web scraping pada beberapa halaman situs dari website yang sama
Dalam banyak kasus, penulisan kode web scraping untuk satu website akan serupa untuk semua halaman web yang ingin Anda scrape. Ini disebabkan oleh keseragaman struktur dan format halaman web yang konsisten di seluruh situs tersebut. Untuk melakukan web scraping dari beberapa halaman dalam satu website yang sama, Anda hanya perlu melakukan looping atau pengulangan untuk mengulangi langkah-langkah sebelumnya untuk setiap halaman.

Contohnya kita akan melakukan webscraping dari tiga halaman yang berasal dari website yang sama, yakni TribunNews.

```{r, echo=TRUE}
# Tambahkan 2 webpages yang berbeda pada daftar alamat website
urls <- c("https://aceh.tribunnews.com/2020/03/11/teror-harimau-sumatera-belum-mereda-di-subulussalam-giliran-desa-bawan-jadi-sasaran?page=all",
          "https://aceh.tribunnews.com/2020/10/05/harimau-berkeliaran-di-objek-wisata-kempra-dampak-tidak-adanya-kajian-lingkungan-spot-baru",
          "https://aceh.tribunnews.com/2020/03/11/warga-pastikan-harimau-sumatera-yang-kembali-meneror-mangsa-ternak?page=all")

# Membuat dataframe yang berisi alamat url, serta hasil webscraping (Saat ini hasilnya NA)
tribun_scraping <- data.frame("Webpages" = urls, "Tanggal" = NA, "Judul" = NA, "Berita" = NA)
tribun_scraping

# Kemudian lakukan pengulangan (looping) untuk mengekstrak informasi yang diinginkan
for (i in 1:nrow(tribun_scraping)) {
  # Ekstraksi tanggal
  tribun_scraping[i,"Tanggal"] <- rvest::read_html(tribun_scraping[i,"Webpages"]) %>% # Membaca website baris i
    rvest::html_node("time") %>% # Membaca kode tanggal
    rvest::html_text() # Mengubah ke dalam format text
  
  # Ekstraksi judul artikel
  tribun_scraping[i,"Judul"] <- rvest::read_html(tribun_scraping[i,"Webpages"]) %>% # Membaca website baris i
    rvest::html_node("#arttitle") %>% # Membaca kode judul artikel
    rvest::html_text() # Mengubah ke dalam format text
  
  # Ekstraksi isi artikel
  tribun_scraping[i,"Berita"] <- rvest::read_html(tribun_scraping[i,"Webpages"]) %>% # Membaca website baris i
  rvest::html_nodes("p") %>% # Membaca kode isi berita artikel
  rvest::html_text() %>%  # Mengubah ke dalam format text
    {gsub('[\r\n\t\"]', '',.)} %>% # Bersihkan text
    paste(collapse ="") # Gabung semua kalimat dalam satu paragraph
}

# Kemudian cek hasil
tribun_scraping

# Simpan dalam file csv
write.csv(tribun_scraping, "E:/myRpackages/Webscraping_tutorial/docs/Tribun_scraping.csv")
```

Penting untuk dicatat bahwa meskipun penulisan kode pada umumnya sama untuk semua halaman web dalam satu situs, ada kemungkinan adanya variasi atau perubahan dalam struktur HTML pada beberapa halaman. Jika Anda menghadapi perbedaan tersebut, Anda mungkin perlu menyesuaikan kode Anda untuk menangani situasi tersebut secara khusus.

##### iii. Menulis kode untuk web scraping pada beberapa halaman situs dari website yang berbeda
Setiap website umumnya memiliki struktur HTML yang berbeda, sehingga untuk melakukan web scraping pada beberapa website yang berbeda, diperlukan adaptasi pada setiap website untuk mengidentifikasi lokasi kode informasi yang diinginkan menggunakan selektorGadget. Selanjutnya, informasi yang ingin diekstrak dari masing-masing website dapat dilakukan dengan menggunakan pengulangan atau looping yang serupa dengan langkah sebelumnya. Perbedaannya, untuk website yang berbeda, kita perlu menggunakan *alternatination* dengan **if** dan **else** seperti contoh berikut.

```{r, echo=TRUE}
# Membuat list halaman web dari situs yang berbeda
urls <- c("https://aceh.tribunnews.com/2020/03/11/teror-harimau-sumatera-belum-mereda-di-subulussalam-giliran-desa-bawan-jadi-sasaran?page=all",
          "https://aceh.tribunnews.com/2020/10/05/harimau-berkeliaran-di-objek-wisata-kempra-dampak-tidak-adanya-kajian-lingkungan-spot-baru",
          "https://aceh.tribunnews.com/2020/03/11/warga-pastikan-harimau-sumatera-yang-kembali-meneror-mangsa-ternak?page=all",
          "https://regional.kompas.com/read/2018/11/15/16461841/masuk-ke-kampung-kawanan-harimau-sumatera-terkam-3-ekor-kerbau-warga?page=all",
         "https://regional.kompas.com/read/2019/12/07/14385741/ternyata-ini-penyebab-harimau-masuk-ke-permukiman-dan-mangsa-ternak-warga?page=all")

# Membuat dataframe yang berisi nama media penerbit, alamat url, serta hasil webscraping (Saat ini hasilnya NA)
web_scraping <- data.frame("Media" = c("Tribunnews", "Tribunnews", "Tribunnews", "Kompas", "Kompas"), 
                           "Webpages" = urls, "Tanggal" = NA, "Judul" = NA, "Berita" = NA)
web_scraping

# Kemudian lakukan pengulangan (looping) untuk mengekstrak informasi yang diinginkan
# dan gunakan if dan else untuk menyesuaikan kode pada website yang berbeda
for (i in 1:nrow(web_scraping)) {
  # Untuk Media Tribunnews, gunakan kode berikut
  if(web_scraping$Media[i]=="Tribunnews") {
    # Ekstraksi tanggal
  web_scraping[i,"Tanggal"] <- rvest::read_html(web_scraping[i,"Webpages"]) %>% # Membaca website baris i
    rvest::html_node("time") %>% # Membaca kode tanggal
    rvest::html_text() # Mengubah ke dalam format text
  
  # Ekstraksi judul artikel
  web_scraping[i,"Judul"] <- rvest::read_html(web_scraping[i,"Webpages"]) %>% # Membaca website baris i
    rvest::html_node("#arttitle") %>% # Membaca kode judul artikel
    rvest::html_text() # Mengubah ke dalam format text
  
  # Ekstraksi isi artikel
  web_scraping[i,"Berita"] <- rvest::read_html(web_scraping[i,"Webpages"]) %>% # Membaca website baris i
  rvest::html_nodes("p") %>% # Membaca kode isi berita artikel
  rvest::html_text() %>%  # Mengubah ke dalam format text
    {gsub('[\r\n\t\"]', '',.)} %>% # Bersihkan text
    paste(collapse ="") # Gabung semua kalimat dalam satu paragraph
  }
  # Untuk media kompas, gunakan kode berikut
  else if(web_scraping$Media[i]=="Kompas"){
    # Ekstraksi tanggal
  web_scraping[i,"Tanggal"] <- rvest::read_html(web_scraping[i,"Webpages"]) %>% # Membaca website baris i
    rvest::html_node(".read__time") %>% # Membaca kode tanggal
    rvest::html_text() # Mengubah ke dalam format text
  
  # Ekstraksi judul artikel
  web_scraping[i,"Judul"] <- rvest::read_html(web_scraping[i,"Webpages"]) %>% # Membaca website baris i
    rvest::html_node(".read__title") %>% # Membaca kode judul artikel
    rvest::html_text() # Mengubah ke dalam format text
  
  # Ekstraksi isi artikel
  web_scraping[i,"Berita"] <- rvest::read_html(web_scraping[i,"Webpages"]) %>% # Membaca website baris i
  rvest::html_nodes("p") %>% # Membaca kode isi berita artikel
  rvest::html_text() %>%  # Mengubah ke dalam format text
    {gsub('[\r\n\t\"]', '',.)} %>% # Bersihkan text
    paste(collapse ="") # Gabung semua kalimat dalam satu paragraph
  }
  # Jika bukan Tribun dan Kompas, stop!!
  else { stop('Media bukan Tribun atau Kompas')}
}

# Kemudian cek hasil
web_scraping

# Simpan dalam file csv
write.csv(web_scraping, "E:/myRpackages/Webscraping_tutorial/docs/web_scraping.csv")
```
Dengan menggunakan model contoh di atas, Anda sekarang dapat melakukan web scraping pada beberapa halaman web, baik dari website yang sama maupun website yang berbeda. Setelah data terkumpul, Anda dapat melanjutkan dengan menganalisis data tersebut.

## V. Pembersihan data hasil web scraping
Data yang dikumpulkan melalui web scraping perlu melalui proses pembersihan sebelum analisis lebih lanjut. Pembersihan ini melibatkan penghapusan karakter khusus seperti hashtag atau simbol yang tidak diinginkan, normalisasi teks seperti mengubah semua huruf menjadi huruf kecil yang konsisten atau mengembalikan kata-kata ke bentuk dasar, penghapusan data duplikat, validasi data, penghapusan spasi, dan penghapusan kata-kata umum yang tidak memberikan makna jika digunakan sendiri, seperti 'dan', 'atau', 'dari', 'ke', dan lainnya. Daftar *stopwords* untuk bahasa Indonesia dapat di download pada [link github ini](https://github.com/masdevid/ID-Stopwords)

Tahapan selanjutnya adalah membersihkan data hasil web scraping, terutama pada bagian isi berita di mana informasi yang akan dianalisis terdapat.
```{r, warning=FALSE}
# Memanggil contoh data yang akan dibersihkan yang merupakan gabungan dari beberapa isi berita
teks_kotor <- paste(web_scraping$Berita[1], web_scraping$Berita[2], web_scraping$Berita[3], 
                    web_scraping$Berita[4], web_scraping$Berita[5]) 

# Merubah semua teks menjadi huruf kecil
teks_bersih_kecil <- tolower(teks_kotor)

# Menghapus tanda baca
teks_bersih_tandabaca <- gsub("[[:punct:]]", "", teks_bersih_kecil)

# Menghapus angka
teks_bersih_angka <- gsub("\\d+", "", teks_bersih_tandabaca)

# Menghapus ekstra spasi
teks_bersih_spasi <- trimws(teks_bersih_angka)

# Menghapus stopWords
# Memanggil stopwords dalam bahasa Indonesia
# download pada link [ini](https://github.com/masdevid/ID-Stopwords)
stopwords_id <- readLines("docs/id.stopwords.02.01.2016.txt") 

# Merubah teks menjadi individual kata
teks_bersih_vektor <- unlist(strsplit(teks_bersih_spasi, "\\s+"))

# Menghapus stopwords dari barisan kata
teks_bersih_stopwords <- teks_bersih_vektor[!teks_bersih_vektor %in% stopwords_id]

# Menggabung kembali barisan kata menjadi kalimat dan paragrap
teks_bersih <- paste(teks_bersih_stopwords, collapse = " ")

# Melihat teks yang telah dibersihkan
teks_bersih
```
## VI. Beberapa contoh analisis data yang dapat dilakukan dengan menggunakan web scraping
### a. Frequensi kata
Dari data yang sudah bersih, kita akan menghitung seberapa sering suatu kata muncul di dalam teks tersebut.
```{r}
# Memisahkan teks menjadi kata-kata individu
teks <- unlist(strsplit(teks_bersih, " "))

# Menghitung frekuensi setiap kata
frekuensi_kata <- table(teks)

# Mengurutkan frekuensi kata secara menurun
frekuensi_terurut <- sort(frekuensi_kata, decreasing = TRUE)

# Menampilkan 10 kata paling sering muncul
kata_teratas <- head(frekuensi_terurut, 10)
kata_teratas

# Konversi ke format dataframe
kata_terbanyak <- as.data.frame(kata_teratas)
```
Selanjutnya, kita dapat membuat visualisasi plot untuk melihat kata yang paling sering muncul dalam teks.
```{r Grafik frekuensi kata, out.width='80%', fig.align='center', fig.cap='Kata dengan frequensi paling banyak', echo=TRUE, warning=TRUE}
# Visualisasi data
barplot(kata_terbanyak$Freq, names.arg = kata_terbanyak$teks, col="skyblue", las=2,
        main="Kata dengan frequensi terbanyak", ylab = "Frekuensi")
```
### b. Word cloud
Anda juga dapat membuat word cloud atau awan kata untuk memvisualisasikan atau memperlihatkan frequensi kemunculan kata-kata dalam suatu teks dengan cara yang menarik dan mudah dipahami.
```{r Grafik awan kata, out.width='60%', fig.align='center', fig.cap='Worcloud', echo=TRUE, warning=FALSE, message=FALSE}
# Memuat library yang diperlukan
library(wordcloud)

# Membuat word cloud
wordcloud(kata_terbanyak$teks, kata_terbanyak$Freq, scale = c(5, 0.5), min.freq = 10, random.order = FALSE,
          colors = brewer.pal(8, "Dark2"), rot.per = 0.35)
```
### c. Analisis sentimen
Contoh analisis selanjutnya adalah analisis sentimen yang bertujuan untuk mengidentifikasi sentimen atau nada emosional yang dapat berupa sentimen positif, negatif, atau netral yang terungkap dalam teks yang telah di-scrape.
```{r, echo=TRUE}
# Memuat library yang diperlukan
library(tidytext)
library(sentimentr)

# Analysis sentiment
sentiment_scores <- sentiment(teks_bersih)

# Print sentiment scores
print(sentiment_scores)
```
Skor hasil analisis sentimen mencerminkan polaritas sentimen teks, menunjukkan apakah teks memiliki sentimen positif atau negatif. Skor tersebut umumnya berupa nilai numerik yang berkisar antara nilai negatif dan nilai positif. Berikut ini interpretasi umum skor sentimen:

Sentimen Positif: Jika skor sentimen mendekati 1 atau bernilai positif, itu menunjukkan bahwa teks memiliki sentimen positif. Semakin tinggi skornya, semakin kuat sentimen positifnya.

Sentimen Negatif: Jika skor sentimen mendekati -1 atau bernilai negatif, itu menunjukkan bahwa teks memiliki sentimen negatif. Semakin rendah skornya, semakin kuat sentimen negatifnya.

Sentimen Netral: Jika skor sentimen mendekati 0, itu mengindikasikan bahwa teks netral dan tidak memiliki sentimen positif atau negatif yang kuat.

Perlu diingat bahwa berbagai pustaka atau model analisis sentimen mungkin menggunakan skala atau metode penilaian yang berbeda. Oleh karena itu, disarankan untuk merujuk pada dokumentasi atau panduan khusus dari pustaka atau model analisis sentimen yang Anda gunakan untuk interpretasi yang lebih akurat terkait skor sentimen.

Selain itu, analisis sentimen tidak selalu sempurna dan dapat memiliki keterbatasan, terutama ketika menangani teks yang nuansanya atau bergantung pada konteks. Selalu merupakan praktik yang baik untuk meninjau hasilnya dan mempertimbangkan konteks yang lebih luas serta pengetahuan domain saat menginterpretasikan skor analisis sentimen.

### d. Analisis konten
Analisis konten juga dapat dilakukan pada teks yang sudah di scraping yang bertujuan untuk memeriksa isi teks atau mengektrak informasi yang dibutuhkan. Contohnya informasi produk, harga, lokasi, ulasan, dan opini. Dalam konteks konservasi KEHATI, analisis konten bisa membantu dalam mengumpulkan data seperti lokasi terjadinya konflik manusia dan harimau sebagai bahan untuk mitigasi konflik. Sebagai contoh, kita akan melakukan ekstraksi lokasi dimana konflik terjadi.
```{r, message=FALSE, warning=FALSE, echo=TRUE}
# Membuat kumpulan kata yang akan diekstrak
list_desa <- c('bawan', 'kute keramil', 'leubok pusaka', 'bengkelang')

# Merubah struktur teks menjadi beberapa suku kata atau urutan kata
teks_suku_kata <- teks_bersih[1] %>% # Pilih teks dari website 1 yang sudah bersih
  corpus::term_stats(ngrams = 1:2) %>% # Membuat list potensi suku kata
  dplyr::select("term") # Ekstrak kolom term saja

# Cek hasil
teks_suku_kata

# Ekstraksi nama desa yang dicari yang terdapat pada teks pada website pertama
# Memanggil pustaka stringdist
library(stringdist)
desa_teks <- tidyr::crossing(teks_suku_kata, list_desa) %>% #
  as.data.frame() %>% 
  # Hitung kesamaan kata (similarity)
  dplyr::mutate('kesamaan' = stringdist::stringsim(term, list_desa)) %>% 
  # Filter kata yang memiliki tingkat kesamaan tinggi
  dplyr::filter(kesamaan >= 0.8)

# Tampilkan hasil
desa_teks
```
Beberapa contoh di atas tentunya merupakan contoh sederhana karena untuk mendapatkan informasi yang lebih terperinci dan akurat dari artikel yang tidak terstruktur, diperlukan analisis dan proses yang lebih kompleks. Beragam tantangan juga muncul, seperti ketidakkonsistenan unit (cth: seribu, 1000, 1K), kesalahan penulisan nama tempat oleh penulis artikel (Tapaktuan menjadi Tapak tuan), dan gangguan dari tautan-tautan yang terekam saat web-scraping.

Namun, secara umum, dengan melakukan web scraping dan teks mining, Anda dapat memperoleh data dan informasi penting tanpa harus pergi ke lapangan atau mengeluarkan biaya tambahan selain biaya listrik dan internet :)

